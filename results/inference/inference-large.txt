You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
The preditive performance on test data of iTunes-Amazon with lora is: f1 0.9629629629629629, acc 0.981651376146789
The inference time on test data of iTunes-Amazon with lora in token level is 0.00010457273605593539
The preditive performance on test data of Beer with lora is: f1 0.9333333333333333, acc 0.978021978021978
The inference time on test data of Beer with lora in token level is 0.00021553138206745017
The preditive performance on test data of Fodors-Zagats with lora is: f1 1.0, acc 1.0
The inference time on test data of Fodors-Zagats with lora in token level is 0.00014747962475721274
The preditive performance on test data of Walmart-Amazon with lora is: f1 0.8264058679706602, acc 0.9653489507076622
The inference time on test data of Walmart-Amazon with lora in token level is 0.00019509602939250117
The preditive performance on test data of Amazon-Google with lora is: f1 0.7421150278293135, acc 0.9393807239424335
The inference time on test data of Amazon-Google with lora in token level is 0.00020333948905521765
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with lora is: f1 0.9680264608599779, acc 0.9882733522038011
The inference time on test data of DBLP-ACM with lora in token level is 0.00011737978209805228
The preditive performance on test data of DBLP-GoogleScholar with lora is: f1 0.951131221719457, acc 0.9811912225705329
The inference time on test data of DBLP-GoogleScholar with lora in token level is 0.00013380841576781538
The preditive performance on test data of Hospital with lora is: f1 0.9945235487404162, acc 0.9997076194374598
The inference time on test data of Hospital with lora in token level is 0.0007840305224349784
The preditive performance on test data of Buy with lora is: f1 0.0, acc 0.9384615384615385
The inference time on test data of Buy with lora in token level is 0.0003095872195283901
The preditive performance on test data of Restaurant with lora is: f1 0.0, acc 0.9069767441860465
The inference time on test data of Restaurant with lora in token level is 0.00045403045404558007
The preditive performance on test data of iTunes-Amazon with p-tune is: f1 0.9642857142857143, acc 0.981651376146789
The inference time on test data of iTunes-Amazon with p-tune in token level is 8.722051682321753e-05
The preditive performance on test data of Beer with p-tune is: f1 0.0, acc 0.8461538461538461
The inference time on test data of Beer with p-tune in token level is 0.00016328225799599523
The preditive performance on test data of Fodors-Zagats with p-tune is: f1 0.7804878048780488, acc 0.9523809523809523
The inference time on test data of Fodors-Zagats with p-tune in token level is 0.0001222313394702897
The preditive performance on test data of Walmart-Amazon with p-tune is: f1 0.8623376623376622, acc 0.9741337237676916
The inference time on test data of Walmart-Amazon with p-tune in token level is 0.00015044029126229432
The preditive performance on test data of Amazon-Google with p-tune is: f1 0.7388781431334622, acc 0.9411251635412123
The inference time on test data of Amazon-Google with p-tune in token level is 0.00015629358792158327
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with p-tune is: f1 0.937720329024677, acc 0.978568540234533
The inference time on test data of DBLP-ACM with p-tune in token level is 9.54960758660022e-05
The preditive performance on test data of DBLP-GoogleScholar with p-tune is: f1 0.8090062111801244, acc 0.9143155694879833
The inference time on test data of DBLP-GoogleScholar with p-tune in token level is 0.00010727737676323553
The preditive performance on test data of Hospital with p-tune is: f1 0.952054794520548, acc 0.9975440032746623
The inference time on test data of Hospital with p-tune in token level is 0.0005985881754391559
The preditive performance on test data of Buy with p-tune is: f1 0.0, acc 0.8615384615384616
The inference time on test data of Buy with p-tune in token level is 0.00023707014815178335
The preditive performance on test data of Restaurant with p-tune is: f1 0.0, acc 0.7558139534883721
The inference time on test data of Restaurant with p-tune in token level is 0.00034838454734811624
The preditive performance on test data of iTunes-Amazon with prefix is: f1 0.9433962264150944, acc 0.9724770642201835
The inference time on test data of iTunes-Amazon with prefix in token level is 8.501688172411069e-05
The preditive performance on test data of Beer with prefix is: f1 0.8666666666666666, acc 0.9560439560439561
The inference time on test data of Beer with prefix in token level is 0.00017739808726889269
The preditive performance on test data of Fodors-Zagats with prefix is: f1 1.0, acc 1.0
The inference time on test data of Fodors-Zagats with prefix in token level is 0.00012050804164572285
The preditive performance on test data of Walmart-Amazon with prefix is: f1 0.8208955223880596, acc 0.9648609077598829
The inference time on test data of Walmart-Amazon with prefix in token level is 0.00016009693077245445
The preditive performance on test data of Amazon-Google with prefix is: f1 0.72265625, acc 0.9380723942433493
The inference time on test data of Amazon-Google with prefix in token level is 0.0001668108003176214
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with prefix is: f1 0.9615806805708014, acc 0.985847149211484
The inference time on test data of DBLP-ACM with prefix in token level is 9.653355774600077e-05
The preditive performance on test data of DBLP-GoogleScholar with prefix is: f1 0.9418874941887495, acc 0.9782305816788576
The inference time on test data of DBLP-GoogleScholar with prefix in token level is 0.00011027157357711257
The preditive performance on test data of Hospital with prefix is: f1 0.952054794520548, acc 0.9975440032746623
The inference time on test data of Hospital with prefix in token level is 0.0006482492403394463
The preditive performance on test data of Buy with prefix is: f1 0.0, acc 0.9230769230769231
The inference time on test data of Buy with prefix in token level is 0.0002548889088798834
The preditive performance on test data of Restaurant with prefix is: f1 0.0, acc 0.7325581395348837
The inference time on test data of Restaurant with prefix in token level is 0.00037478835952174075
The preditive performance on test data of iTunes-Amazon with prompt is: f1 0.7812500000000001, acc 0.8715596330275229
The inference time on test data of iTunes-Amazon with prompt in token level is 8.555258582353483e-05
The preditive performance on test data of Beer with prompt is: f1 0.7586206896551724, acc 0.9230769230769231
The inference time on test data of Beer with prompt in token level is 0.00016429018091272424
The preditive performance on test data of Fodors-Zagats with prompt is: f1 0.9767441860465117, acc 0.9947089947089947
The inference time on test data of Fodors-Zagats with prompt in token level is 0.0001199515819151963
The preditive performance on test data of Walmart-Amazon with prompt is: f1 0.8616187989556136, acc 0.9741337237676916
The inference time on test data of Walmart-Amazon with prompt in token level is 0.00015109928802148657
The preditive performance on test data of Amazon-Google with prompt is: f1 0.7643564356435643, acc 0.948102921936328
The inference time on test data of Amazon-Google with prompt in token level is 0.0001549851722229651
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with prompt is: f1 0.9832402234636871, acc 0.9939344925192074
The inference time on test data of DBLP-ACM with prompt in token level is 9.344991411211e-05
The preditive performance on test data of DBLP-GoogleScholar with prompt is: f1 0.9592592592592593, acc 0.9846743295019157
The inference time on test data of DBLP-GoogleScholar with prompt in token level is 0.00010570374880981968
The preditive performance on test data of Hospital with prompt is: f1 0.9161747343565526, acc 0.9958481960119291
The inference time on test data of Hospital with prompt in token level is 0.0005997311208244459
The preditive performance on test data of Buy with prompt is: f1 0.0, acc 0.8307692307692308
The inference time on test data of Buy with prompt in token level is 0.0002365011653687634
The preditive performance on test data of Restaurant with prompt is: f1 0.0, acc 0.37209302325581395
The inference time on test data of Restaurant with prompt in token level is 0.0003489648918907891
