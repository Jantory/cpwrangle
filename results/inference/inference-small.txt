You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
The preditive performance on test data of iTunes-Amazon with lora is: f1 0.9629629629629629, acc 0.981651376146789
The inference time on test data of iTunes-Amazon with lora in token level is 3.558318887948655e-05
The preditive performance on test data of Beer with lora is: f1 0.9333333333333333, acc 0.978021978021978
The inference time on test data of Beer with lora in token level is 7.343717309554722e-05
The preditive performance on test data of Fodors-Zagats with lora is: f1 0.9767441860465117, acc 0.9947089947089947
The inference time on test data of Fodors-Zagats with lora in token level is 4.954902956636762e-05
The preditive performance on test data of Walmart-Amazon with lora is: f1 0.7918781725888324, acc 0.9599804782820889
The inference time on test data of Walmart-Amazon with lora in token level is 6.893748150515468e-05
The preditive performance on test data of Amazon-Google with lora is: f1 0.7246376811594204, acc 0.9419973833406018
The inference time on test data of Amazon-Google with lora in token level is 6.879724594348544e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with lora is: f1 0.9875706214689265, acc 0.9955519611807522
The inference time on test data of DBLP-ACM with lora in token level is 3.9771977578989915e-05
The preditive performance on test data of DBLP-GoogleScholar with lora is: f1 0.9526022304832714, acc 0.9822361546499477
The inference time on test data of DBLP-GoogleScholar with lora in token level is 4.48818109334519e-05
The preditive performance on test data of Hospital with lora is: f1 0.9484536082474228, acc 0.9973685749371382
The inference time on test data of Hospital with lora in token level is 0.00025757580428237146
The preditive performance on test data of Buy with lora is: f1 0.0, acc 0.9230769230769231
The inference time on test data of Buy with lora in token level is 0.00010531462617024291
The preditive performance on test data of Restaurant with lora is: f1 0.0, acc 0.8953488372093024
The inference time on test data of Restaurant with lora in token level is 0.00016066676717192856
The preditive performance on test data of iTunes-Amazon with p-tune is: f1 0.9019607843137256, acc 0.9541284403669725
The inference time on test data of iTunes-Amazon with p-tune in token level is 2.7585756233614623e-05
The preditive performance on test data of Beer with p-tune is: f1 0.7857142857142857, acc 0.9340659340659341
The inference time on test data of Beer with p-tune in token level is 5.751458773363291e-05
The preditive performance on test data of Fodors-Zagats with p-tune is: f1 0.9523809523809523, acc 0.9894179894179894
The inference time on test data of Fodors-Zagats with p-tune in token level is 3.958197550954601e-05
The preditive performance on test data of Walmart-Amazon with p-tune is: f1 0.6380697050938339, acc 0.9341142020497804
The inference time on test data of Walmart-Amazon with p-tune in token level is 5.354118155987017e-05
The preditive performance on test data of Amazon-Google with p-tune is: f1 0.6762295081967213, acc 0.9310946358482337
The inference time on test data of Amazon-Google with p-tune in token level is 5.3970431278765156e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with p-tune is: f1 0.9875141884222476, acc 0.9955519611807522
The inference time on test data of DBLP-ACM with p-tune in token level is 3.132373834437623e-05
The preditive performance on test data of DBLP-GoogleScholar with p-tune is: f1 0.948938093086308, acc 0.9803204458376872
The inference time on test data of DBLP-GoogleScholar with p-tune in token level is 3.540794918946414e-05
The preditive performance on test data of Hospital with p-tune is: f1 0.9406593406593406, acc 0.9968422899245658
The inference time on test data of Hospital with p-tune in token level is 0.00020649342773259555
The preditive performance on test data of Buy with p-tune is: f1 0.0, acc 0.8307692307692308
The inference time on test data of Buy with p-tune in token level is 8.23008359776286e-05
The preditive performance on test data of Restaurant with p-tune is: f1 0.0, acc 0.3488372093023256
The inference time on test data of Restaurant with p-tune in token level is 0.00012768970336504934
The preditive performance on test data of iTunes-Amazon with prefix is: f1 0.8813559322033898, acc 0.9357798165137615
The inference time on test data of iTunes-Amazon with prefix in token level is 2.8826823375558616e-05
The preditive performance on test data of Beer with prefix is: f1 0.6956521739130435, acc 0.9230769230769231
The inference time on test data of Beer with prefix in token level is 6.041955521584775e-05
The preditive performance on test data of Fodors-Zagats with prefix is: f1 0.9777777777777777, acc 0.9947089947089947
The inference time on test data of Fodors-Zagats with prefix in token level is 4.2045688154978606e-05
The preditive performance on test data of Walmart-Amazon with prefix is: f1 0.0, acc 0.9058077110785749
The inference time on test data of Walmart-Amazon with prefix in token level is 5.469973813001822e-05
The preditive performance on test data of Amazon-Google with prefix is: f1 0.49145299145299143, acc 0.8979502834714348
The inference time on test data of Amazon-Google with prefix in token level is 5.6492450697967805e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with prefix is: f1 0.9284210526315789, acc 0.9725030327537404
The inference time on test data of DBLP-ACM with prefix in token level is 3.3503522228934204e-05
The preditive performance on test data of DBLP-GoogleScholar with prefix is: f1 0.917883211678832, acc 0.9686520376175548
The inference time on test data of DBLP-GoogleScholar with prefix in token level is 3.848021696125215e-05
The preditive performance on test data of Hospital with prefix is: f1 0.33393829401088926, acc 0.9785392667095492
The inference time on test data of Hospital with prefix in token level is 0.00021954936977930124
The preditive performance on test data of Buy with prefix is: f1 0.0, acc 0.9230769230769231
The inference time on test data of Buy with prefix in token level is 8.97496679766734e-05
The preditive performance on test data of Restaurant with prefix is: f1 0.0, acc 0.6162790697674418
The inference time on test data of Restaurant with prefix in token level is 0.00013322886668454468
The preditive performance on test data of iTunes-Amazon with prompt is: f1 0.9152542372881356, acc 0.9541284403669725
The inference time on test data of iTunes-Amazon with prompt in token level is 2.7017340839409965e-05
The preditive performance on test data of Beer with prompt is: f1 0.896551724137931, acc 0.967032967032967
The inference time on test data of Beer with prompt in token level is 5.6720261068331936e-05
The preditive performance on test data of Fodors-Zagats with prompt is: f1 1.0, acc 1.0
The inference time on test data of Fodors-Zagats with prompt in token level is 3.891643418026127e-05
The preditive performance on test data of Walmart-Amazon with prompt is: f1 0.7575057736720554, acc 0.9487554904831625
The inference time on test data of Walmart-Amazon with prompt in token level is 5.142493786478855e-05
The preditive performance on test data of Amazon-Google with prompt is: f1 0.7125506072874495, acc 0.9380723942433493
The inference time on test data of Amazon-Google with prompt in token level is 5.4254794400595726e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with prompt is: f1 0.9887133182844244, acc 0.9959563283461383
The inference time on test data of DBLP-ACM with prompt in token level is 3.1139461144198254e-05
The preditive performance on test data of DBLP-GoogleScholar with prompt is: f1 0.9525564256103178, acc 0.9820619993033786
The inference time on test data of DBLP-GoogleScholar with prompt in token level is 3.519134855763093e-05
The preditive performance on test data of Hospital with prompt is: f1 0.7560975609756097, acc 0.9894742997485527
The inference time on test data of Hospital with prompt in token level is 0.00020312723216379185
The preditive performance on test data of Buy with prompt is: f1 0.0, acc 0.8769230769230769
The inference time on test data of Buy with prompt in token level is 8.319560630694618e-05
The preditive performance on test data of Restaurant with prompt is: f1 0.0, acc 0.3372093023255814
The inference time on test data of Restaurant with prompt in token level is 0.00012816177268438987
