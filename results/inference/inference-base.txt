You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
The preditive performance on test data of iTunes-Amazon with lora is: f1 0.9615384615384615, acc 0.981651376146789
The inference time on test data of iTunes-Amazon with lora in token level is 5.729006318151816e-05
The preditive performance on test data of Beer with lora is: f1 0.9032258064516129, acc 0.967032967032967
The inference time on test data of Beer with lora in token level is 0.00012214677696179309
The preditive performance on test data of Fodors-Zagats with lora is: f1 1.0, acc 1.0
The inference time on test data of Fodors-Zagats with lora in token level is 8.231708293000734e-05
The preditive performance on test data of Walmart-Amazon with lora is: f1 0.8638743455497383, acc 0.974621766715471
The inference time on test data of Walmart-Amazon with lora in token level is 0.0001108557274617999
The preditive performance on test data of Amazon-Google with lora is: f1 0.7248576850094877, acc 0.9367640645442652
The inference time on test data of Amazon-Google with lora in token level is 0.0001154378427738742
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with lora is: f1 0.9820224719101124, acc 0.9935301253538212
The inference time on test data of DBLP-ACM with lora in token level is 6.549479708060725e-05
The preditive performance on test data of DBLP-GoogleScholar with lora is: f1 0.9563197026022305, acc 0.9836293974225009
The inference time on test data of DBLP-GoogleScholar with lora in token level is 7.581492363743065e-05
The preditive performance on test data of Hospital with lora is: f1 0.9544419134396355, acc 0.9976609554996784
The inference time on test data of Hospital with lora in token level is 0.0004400888860706113
The preditive performance on test data of Buy with lora is: f1 0.0, acc 0.9384615384615385
The inference time on test data of Buy with lora in token level is 0.00017915485607850263
The preditive performance on test data of Restaurant with lora is: f1 0.0, acc 0.9186046511627907
The inference time on test data of Restaurant with lora in token level is 0.0002622261831526276
The preditive performance on test data of iTunes-Amazon with p-tune is: f1 0.8852459016393442, acc 0.9357798165137615
The inference time on test data of iTunes-Amazon with p-tune in token level is 4.564139189842615e-05
The preditive performance on test data of Beer with p-tune is: f1 0.7777777777777778, acc 0.9120879120879121
The inference time on test data of Beer with p-tune in token level is 9.790573997059087e-05
The preditive performance on test data of Fodors-Zagats with p-tune is: f1 1.0, acc 1.0
The inference time on test data of Fodors-Zagats with p-tune in token level is 6.394936154061805e-05
The preditive performance on test data of Walmart-Amazon with p-tune is: f1 0.8108108108108106, acc 0.9658369936554417
The inference time on test data of Walmart-Amazon with p-tune in token level is 8.472015616330581e-05
The preditive performance on test data of Amazon-Google with p-tune is: f1 0.6991869918699187, acc 0.935455734845181
The inference time on test data of Amazon-Google with p-tune in token level is 8.880936565326774e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with p-tune is: f1 0.9832026875699889, acc 0.9939344925192074
The inference time on test data of DBLP-ACM with p-tune in token level is 5.131341376394596e-05
The preditive performance on test data of DBLP-GoogleScholar with p-tune is: f1 0.9552376557452701, acc 0.9831069313827935
The inference time on test data of DBLP-GoogleScholar with p-tune in token level is 6.004856446293341e-05
The preditive performance on test data of Hospital with p-tune is: f1 0.9544419134396355, acc 0.9976609554996784
The inference time on test data of Hospital with p-tune in token level is 0.00034912596270633673
The preditive performance on test data of Buy with p-tune is: f1 0.0, acc 0.8615384615384616
The inference time on test data of Buy with p-tune in token level is 0.00013938048104499974
The preditive performance on test data of Restaurant with p-tune is: f1 0.0, acc 0.5813953488372093
The inference time on test data of Restaurant with p-tune in token level is 0.00020441480514497387
The preditive performance on test data of iTunes-Amazon with prefix is: f1 0.9019607843137256, acc 0.9541284403669725
The inference time on test data of iTunes-Amazon with prefix in token level is 4.8227474149119006e-05
The preditive performance on test data of Beer with prefix is: f1 0.8571428571428571, acc 0.9560439560439561
The inference time on test data of Beer with prefix in token level is 0.0001018550234583907
The preditive performance on test data of Fodors-Zagats with prefix is: f1 1.0, acc 1.0
The inference time on test data of Fodors-Zagats with prefix in token level is 6.828546369426619e-05
The preditive performance on test data of Walmart-Amazon with prefix is: f1 0.6849999999999999, acc 0.9385065885797951
The inference time on test data of Walmart-Amazon with prefix in token level is 9.17119721764113e-05
The preditive performance on test data of Amazon-Google with prefix is: f1 0.6573146292585169, acc 0.9254252071522023
The inference time on test data of Amazon-Google with prefix in token level is 9.640110583888066e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with prefix is: f1 0.9720670391061451, acc 0.9898908208653457
The inference time on test data of DBLP-ACM with prefix in token level is 5.525398514831907e-05
The preditive performance on test data of DBLP-GoogleScholar with prefix is: f1 0.9467345993515517, acc 0.979972135144549
The inference time on test data of DBLP-GoogleScholar with prefix in token level is 6.40789232222996e-05
The preditive performance on test data of Hospital with prefix is: f1 0.9123222748815166, acc 0.995672767674405
The inference time on test data of Hospital with prefix in token level is 0.000370437991371737
The preditive performance on test data of Buy with prefix is: f1 0.0, acc 0.9230769230769231
The inference time on test data of Buy with prefix in token level is 0.00014718522277819553
The preditive performance on test data of Restaurant with prefix is: f1 0.0, acc 0.5930232558139535
The inference time on test data of Restaurant with prefix in token level is 0.0002200142416444228
The preditive performance on test data of iTunes-Amazon with prompt is: f1 0.9310344827586207, acc 0.963302752293578
The inference time on test data of iTunes-Amazon with prompt in token level is 4.47498906711076e-05
The preditive performance on test data of Beer with prompt is: f1 0.7200000000000001, acc 0.9230769230769231
The inference time on test data of Beer with prompt in token level is 9.59372733592378e-05
The preditive performance on test data of Fodors-Zagats with prompt is: f1 0.9, acc 0.9788359788359788
The inference time on test data of Fodors-Zagats with prompt in token level is 6.452539926446879e-05
The preditive performance on test data of Walmart-Amazon with prompt is: f1 0.7650000000000001, acc 0.954123962908736
The inference time on test data of Walmart-Amazon with prompt in token level is 8.46034959531173e-05
The preditive performance on test data of Amazon-Google with prompt is: f1 0.702258726899384, acc 0.9367640645442652
The inference time on test data of Amazon-Google with prompt in token level is 8.838160396542809e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with prompt is: f1 0.9844097995545658, acc 0.9943388596845936
The inference time on test data of DBLP-ACM with prompt in token level is 5.126523284949991e-05
The preditive performance on test data of DBLP-GoogleScholar with prompt is: f1 0.9536909674461256, acc 0.9824103099965169
The inference time on test data of DBLP-GoogleScholar with prompt in token level is 5.978365946749734e-05
The preditive performance on test data of Hospital with prompt is: f1 0.966702470461869, acc 0.9981872405122507
The inference time on test data of Hospital with prompt in token level is 0.0003482309219420539
The preditive performance on test data of Buy with prompt is: f1 0.0, acc 0.8615384615384616
The inference time on test data of Buy with prompt in token level is 0.00013678568364603198
The preditive performance on test data of Restaurant with prompt is: f1 0.0, acc 0.3372093023255814
The inference time on test data of Restaurant with prompt in token level is 0.00020918759829824084
The preditive performance on test data of iTunes-Amazon with finetune is: f1 0.912280701754386, acc 0.9541284403669725
The inference time on test data of iTunes-Amazon with finetune in token level is 4.54266158814054e-05
The preditive performance on test data of Beer with finetune is: f1 0.9333333333333333, acc 0.978021978021978
The inference time on test data of Beer with finetune in token level is 9.384093156719573e-05
The preditive performance on test data of Fodors-Zagats with finetune is: f1 0.9767441860465117, acc 0.9947089947089947
The inference time on test data of Fodors-Zagats with finetune in token level is 6.406744062258129e-05
The preditive performance on test data of Walmart-Amazon with finetune is: f1 0.7769028871391076, acc 0.9585163494387506
The inference time on test data of Walmart-Amazon with finetune in token level is 8.573626300940497e-05
The preditive performance on test data of Amazon-Google with finetune is: f1 0.689108910891089, acc 0.9315307457479285
The inference time on test data of Amazon-Google with finetune in token level is 8.606500876500632e-05
Token indices sequence length is longer than the specified maximum sequence length for this model (397 > 300). Running this sequence through the model will result in indexing errors
The preditive performance on test data of DBLP-ACM with finetune is: f1 0.9853107344632768, acc 0.9947432268499797
The inference time on test data of DBLP-ACM with finetune in token level is 5.0498505216369326e-05
The preditive performance on test data of DBLP-GoogleScholar with finetune is: f1 0.9397701149425287, acc 0.9771856495994427
The inference time on test data of DBLP-GoogleScholar with finetune in token level is 5.783222578714759e-05
The preditive performance on test data of Hospital with finetune is: f1 0.9496567505720824, acc 0.9974270510496462
The inference time on test data of Hospital with finetune in token level is 0.0003315325851322672
The preditive performance on test data of Buy with finetune is: f1 0.0, acc 0.9538461538461539
The inference time on test data of Buy with finetune in token level is 0.00013325393023233062
The preditive performance on test data of Restaurant with finetune is: f1 0.0, acc 0.9186046511627907
The inference time on test data of Restaurant with finetune in token level is 0.00019887851851037888
